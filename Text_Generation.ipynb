{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "DELIMITER = '|'\n",
    "seq_length = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset.csv', dtype=str, delimiter=DELIMITER).sample(frac=1)\n",
    "lyrics_list = []\n",
    "for index, row in dataset_df.iterrows():\n",
    "    lyrics = row[\"lyrics\"]\n",
    "    if isinstance(lyrics, str):\n",
    "        lyrics_list.append(lyrics.lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "lyrics = ' '.join(lyrics_list)\n",
    "vocab = sorted(set(lyrics))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(_ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(_ids), axis=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([41 40 40 ... 39 44 40], shape=(12945659,), dtype=int8)\n",
      "tf.Tensor(\n",
      "[b'f' b'e' b'e' b'l' b's' b' ' b'l' b'i' b'k' b'e' b' ' b't' b'i' b'm'\n",
      " b'e' b\"'\" b's' b' ' b'm' b'o' b'v' b'i' b'n' b'g' b' ' b'i' b'n' b' '\n",
      " b's' b'l' b'o' b'w' b' ' b'm' b'o' b't' b'i' b'o' b'n' b'\\n' b'j' b'u'\n",
      " b's' b't' b' ' b't' b'r' b'y' b'i' b'n' b'g' b' ' b't' b'o' b' ' b'o'\n",
      " b'c' b'c' b'u' b'p' b'y' b' ' b'm' b'y' b' ' b'm' b'i' b'n' b'd' b'\\n'\n",
      " b's' b'o' b' ' b't' b'h' b'a' b't' b' ' b'i' b' ' b'd' b'o' b'n' b\"'\"\n",
      " b't' b'g' b'o' b'l' b'o' b'o' b'n' b'e' b'y' b' ' b'o' b'v' b'e' b'r'\n",
      " b' ' b'y' b'o'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(lyrics, 'UTF-8'))\n",
    "all_ids = tf.cast(all_ids, dtype=tf.int8)\n",
    "print(all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"feels like time's moving in slow motion\\njust trying to occupy my mind\\nso that i don'tgolooney over yo\"\n",
      "b'u\\njust trying to amplify the sound\\ntodrown out all of this need for you\\nbiting my nails, got me nervo'\n",
      "b\"us, so anxious\\nsee it's one o'clock now\\nnoon felt like three hours ago\\n\\ni just wanna know your e.t.a.\"\n",
      "b\", e.t.a.\\nout the window, got me looking out the street\\nwhat's your e.t.a.?\\ndistance only made us grow\"\n",
      "b\" fonder\\nof one another\\nbe honest, what's your e.t.a.?\\nwhat's your e.t.a.?\\nsay you almost right here n\"\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def split_input_target(sequence: list[str]) -> tuple[list[str], list[str]]:\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int8, name=None), TensorSpec(shape=(64, 100), dtype=tf.int8, name=None))>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    sequences\n",
    "    .map(split_input_target)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.lstm.get_initial_state(x)\n",
    "    x, state1, state2  = self.lstm(x, initial_state=states, training=training)\n",
    "    states = (state1, state2)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model, ABC):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=0.40)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life goes on\n",
      "and i know\n",
      "that i was lost\n",
      "\n",
      "i know you want me\n",
      "i know you wanted me to be the one\n",
      "\n",
      "and i will be a little bit to be a little personal\n",
      "i wish i could find a way to be alone\n",
      "\n",
      "oh, i wish i could find you away\n",
      "i wish the world will go on and on\n",
      "i will be your boyfriend\n",
      "oh, when you walk away\n",
      "oh, when i was a bitter fuck\n",
      "was the last time i told you that i was the one\n",
      "and i wish i was young, the way you move\n",
      "i was broken with your body\n",
      "i want you to know that i wanted you to know\n",
      "that you were the one that i wanted\n",
      "was it all alone?\n",
      "where did we go wrong?\n",
      "where did she go?\n",
      "where did she go?\n",
      "where did she go?\n",
      "where did she go?\n",
      "when did it go?\n",
      "when the world goes to be somebody should be\n",
      "whenever i walk away\n",
      "\n",
      "i was all alone\n",
      "whenever you look at me\n",
      "i know i will sing\n",
      "\n",
      "oh, someone like you\n",
      "you might also like\n",
      "and i know that i don't wanna know\n",
      "i know you want me, eh\n",
      "you know i want ya\n",
      "i know you want me to be my girl\n",
      "i don't wanna be alone in the darkness\n",
      "i don't wanna be alone in the darknes\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "\n",
    "while True:\n",
    "    string_input = input(\"Enter seed text: \")\n",
    "    #string_input = \"i am capable of \"\n",
    "    if string_input == \"\":\n",
    "        break\n",
    "    string_input = tf.constant([string_input])\n",
    "    result = [string_input]\n",
    "    for n in range(1000):\n",
    "      next_char, states = one_step_model.generate_one_step(string_input, states=states)\n",
    "      string_input = string_input + next_char\n",
    "\n",
    "    print(string_input.numpy()[0].decode())\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
