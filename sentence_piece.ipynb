{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DELIMITER = '|'\n",
    "seq_length = 128\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset.csv', dtype=str, delimiter=DELIMITER).sample(frac=1)\n",
    "\n",
    "lyrics = dataset_df['lyrics'].str.cat()\n",
    "lyrics = re.sub(r'\\n{3,}', '\\n', lyrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sentence piece model\r\n",
      "512 unique characters\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(spm_model_file='sp_model.model')\n",
    "print(f'{tokenizer.vocab_size} unique characters')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(lyrics)\n",
    "all_ids = tf.convert_to_tensor(tokens, dtype=tf.int16)\n",
    "print(all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True, name=\"batched_lyrics_as_ids\")\n",
    "sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for seq in sequences.take(1):\n",
    "  print(tokenizer.convert_ids_to_tokens(seq))\n",
    "  print(tokenizer.detokenize(seq))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_input_target(sequence: list[str]) -> tuple[list[str], list[str]]:\n",
    "  input_text = sequence[:-1]\n",
    "  target_text = sequence[1:]\n",
    "  return input_text, target_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    sequences\n",
    "    .map(split_input_target)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from model import MyModel\n",
    "model = MyModel(vocab_size=tokenizer.vocab_size,\n",
    "                embedding_dim=256,\n",
    "                rnn_units=1024)\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adamax()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Weights\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = './model'\n",
    "if os.path.exists(model_dir):\n",
    "    model.load_weights(tf.train.latest_checkpoint(model_dir))\n",
    "    print('Loaded Weights')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS,)\n",
    "model.save_weights(os.path.join(model_dir, \"weights\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(_history):\n",
    "    fig, axs = plt.subplots(1, len(_history.items()), figsize=(10,5))\n",
    "    fig.suptitle(\"Metrics\")\n",
    "    for i, (title, values) in enumerate(_history.items()):\n",
    "        axs[i].plot(values)\n",
    "        axs[i].set_xlabel(\"Epochs\")\n",
    "        axs[i].set_ylabel(title.title())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_graphs(history.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "class OneStep(tf.keras.Model, ABC):\n",
    "    def __init__(self, model, tokenizer: FullTokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = tf.constant([[tokenizer.sp_model.unk_id()]], dtype=tf.int64)\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put an -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[tokenizer.vocab_size])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "        # inputs = tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        # states = [tf.TensorSpec(shape=[None, None], dtype=tf.float32) for _ in range(4)]\n",
    "        # temperature = tf.TensorSpec(shape=None, dtype=tf.float32)\n",
    "        # steps = tf.TensorSpec(shape=None, dtype=tf.int32)\n",
    "        # self.generate.get_concrete_function(inputs, states, temperature, steps)\n",
    "\n",
    "    # @tf.function\n",
    "    def generate(self, inputs: str, states=None, temperature: float = 1.0, steps: int = 200):\n",
    "        tokens = self.tokenizer.tokenize(inputs)\n",
    "        output_array = tf.TensorArray(dtype=tf.int16, dynamic_size=True, size=0)\n",
    "        for i in tf.range(0, len(tokens)):\n",
    "            output_array = output_array.write(output_array.size(), tokens[i])\n",
    "\n",
    "        for _ in tf.range(steps):\n",
    "            inputs = tf.convert_to_tensor([output_array.stack()])\n",
    "            predicted_logits, states = self.model(inputs=inputs, states=states,\n",
    "                                                  return_state=True)\n",
    "            # Only use the last prediction.\n",
    "            predicted_logits = predicted_logits[:, -1, :]\n",
    "            predicted_logits = predicted_logits / temperature\n",
    "            # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "            predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "            # Sample the output logits to generate token IDs.\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "            predicted_id = tf.cast(predicted_ids[0], dtype=tf.int16)\n",
    "            output_array = output_array.write(output_array.size(), predicted_id)\n",
    "\n",
    "        return self.tokenizer.detokenize(output_array.stack())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "one_step = OneStep(model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We know each other tonight\n",
      "Don't say good-bye\n",
      "\n",
      "Someday you'll dream of God\n",
      "We'll always be together\n",
      "And our love is gettin' carted\n",
      "Off together the water will crawl\n",
      "Take in kisses underneath\n",
      "We're livin' in a cheatar end\n",
      "\n",
      "He is prangin'\n",
      "And you'll be glad to say\n",
      "Was ever there to remember\n",
      "That is my pleasure\n",
      "We are just me\n",
      "\n",
      "This might he speaks to you\n",
      "Brother be memory\n",
      "Ha ha, gives can you wear\n",
      "Just please take the end of us\n",
      "The creator's head together\n",
      "\n",
      "Some night will we find you better\n",
      "Some say I'll come inside you\n",
      "Cause day you gonna\n",
      "We know each other is a place I want when I keep running around all in a place where ad behave me still You're all i ever needed heaven meet Thee Your better hands\n",
      "the less of your kingdom suyside it's either pretending he's coming with you even where I need to be\n",
      "there's no escape where no ones around me it can go on because even if U'm gonna give it up for Remain i'm on\n",
      "\n",
      "No struggles come over but there's nothing you should do\n",
      "like* you came with with him be you, how could I kill him boy?\n",
      "well, if your man came to sleep\n",
      "yeah you'd get slapped up and jumped in a bar in his test\n",
      "jumped up from Omaispan, he was never takin over\n"
     ]
    }
   ],
   "source": [
    "test = \"We know each other\"\n",
    "states = None\n",
    "print(one_step.generate(test, steps = 250))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
