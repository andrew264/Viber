{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DELIMITER = '|'\n",
    "seq_length = 128\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset.csv', dtype=str, delimiter=DELIMITER).sample(frac=1)\n",
    "\n",
    "lyrics = dataset_df['lyrics'].str.cat()\n",
    "lyrics = re.sub(r'\\n{3,}', '\\n', lyrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = tf.io.gfile.GFile('sp_model.model', 'rb').read()\n",
    "tokenizer = text.SentencepieceTokenizer(model=model, out_type=tf.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(lyrics)\n",
    "all_ids = tf.convert_to_tensor(tokens, dtype=tf.int32)\n",
    "print(all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True, name=\"batched_lyrics_as_ids\")\n",
    "sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for seq in sequences.take(1):\n",
    "  print(tokenizer.id_to_string(seq))\n",
    "  print(tokenizer.detokenize(seq))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_input_target(sequence: list[str]) -> tuple[list[str], list[str]]:\n",
    "  input_text = sequence[:-1]\n",
    "  target_text = sequence[1:]\n",
    "  return input_text, target_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    sequences\n",
    "    .map(split_input_target)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from model import MyModel\n",
    "model = MyModel(vocab_size=tokenizer.vocab_size(),\n",
    "                embedding_dim=256,\n",
    "                rnn_units=1024)\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adamax()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'], run_eagerly=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Weights\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = './model'\n",
    "if os.path.exists(model_dir):\n",
    "    model.load_weights(tf.train.latest_checkpoint(model_dir))\n",
    "    print('Loaded Weights')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS,)\n",
    "model.save_weights(os.path.join(model_dir, \"weights\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(_history):\n",
    "    fig, axs = plt.subplots(1, len(_history.items()), figsize=(10,5))\n",
    "    fig.suptitle(\"Metrics\")\n",
    "    for i, (title, values) in enumerate(_history.items()):\n",
    "        axs[i].plot(values)\n",
    "        axs[i].set_xlabel(\"Epochs\")\n",
    "        axs[i].set_ylabel(title.title())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_graphs(history.history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "class OneStep(tf.keras.Model, ABC):\n",
    "    def __init__(self, model, tokenizer: text.SentencepieceTokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = tf.constant([[0]], dtype=tf.int64)\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put an -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[tokenizer.vocab_size()])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    def generate(self, inputs: str, temperature: float = 1.0, steps: int = 200, states = None):\n",
    "        tokens = self.tokenizer.tokenize(inputs)\n",
    "        output_array = tf.TensorArray(dtype=tf.int32, dynamic_size=True, size=0)\n",
    "        for i in tf.range(0, len(tokens)):\n",
    "            output_array = output_array.write(output_array.size(), tokens[i])\n",
    "\n",
    "        for _ in tf.range(steps):\n",
    "            inputs = tf.convert_to_tensor([output_array.stack()])\n",
    "            predicted_logits, states = self.model(inputs=inputs, states=states,\n",
    "                                                  return_state=True)\n",
    "            # Only use the last prediction.\n",
    "            predicted_logits = predicted_logits[:, -1, :]\n",
    "            predicted_logits = predicted_logits / temperature\n",
    "            # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "            predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "            # Sample the output logits to generate token IDs.\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "            predicted_id = tf.cast(predicted_ids[0], dtype=tf.int32)\n",
    "            output_array = output_array.write(output_array.size(), predicted_id)\n",
    "\n",
    "        return self.tokenizer.detokenize(output_array.stack())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "one_step = OneStep(model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We know each other thing I want.. They be screamin it out.\n",
      "i know this one thing must go on till it stop me, feel it right, yo' work bow do i need some rappin? i know what i'm talkin is real, it's snaping there aint no star under the light, you're all loose im feelin blown. And dark straight, plus trip flight, momma, we get snow, grab them millionaires pocket, empty, hollow in the rain, im sick bumpin, i'm hot en clothin through the rock..\n",
      "i got that top butt drop in God we got pass smoke to get us all the night waits till i love em to shine in these i.m. bottl\n"
     ]
    }
   ],
   "source": [
    "test = \"We know each other\"\n",
    "states = None\n",
    "print(one_step.generate(test, steps = 250).numpy().decode())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
